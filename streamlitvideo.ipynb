{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" # importing all necessary libraries\n\nimport av\nimport cv2\nimport numpy as np\nimport streamlit as st        \nfrom aiortc.contrib.media import MediaPlayer\nfrom streamlit_webrtc import VideoTransformerBase, webrtc_streamer\nimport cv2\nfrom streamlit_webrtc import VideoTransformerBase, webrtc_streamer     #streamlit-webrtc helps to deal with real-time video streams.\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras import layers\n\n\nmy_model = tf.keras.models.load_model('../input/model5/model.h5')    # loading .h5 file of our model and storing it in my_model\n\nclass VideoTransformer(VideoTransformerBase): \n    def transform(self, frame):                                                            # transform() method, which transforms each frame coming from the video stream.\n        img = frame.to_ndarray(format=\"bgr24\")                                              # coverting captured image into array of pixels\n        face_detect = cv2.CascadeClassifier('../input/model56/haarcascade_frontalface_default.xml')          #  load cascade classifier\n        #eye_detect = cv2.CascadeClassifier('haarcascade_eye.xml')\n        \n        class_labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']        # the prediction will be number from 0-6 ; to link it to its emotion we created this dictionary.\n\n\n\n        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                  # converting image into grayscale\n        face_roi = face_detect.detectMultiScale(img_gray, 1.3,1)          # ROI (region of interest of detected face, stored as tuple of bottom left\n        if face_roi is ():                                                # check if face_roi is empty ie. no face detected\n            return img\n\n        for(x,y,w,h) in face_roi:                                          # iterate through faces and draw rectangle over each face\n            x = x - 5\n            w = w + 10\n            y = y + 7\n            h = h + 2\n            cv2.rectangle(img, (x,y),(x+w,y+h),(125,125,10), 2)           # (x,y)- top left point  ; (x+w,y+h)-bottom right point  ;  (125,125,10)-colour of rectangle ; 2- thickness \n            img_color_crop = img[y:y+h,x:x+w]                             # croping colour image\n            final_image = cv2.resize(img_color_crop, (48,48))           # size of colured image is resized to 224,224\n            final_image = np.expand_dims(final_image, axis = 0)           # array is expanded by inserting axis at position 0\n            final_image = final_image/255.0                               # feature scaling of final image\n            prediction = my_model.predict(final_image)                    # emotion of the captured image is detected with the help of our model\n            label=class_labels[prediction.argmax()]                       # we find the label of class which has maximaum probalility \n            cv2.putText(img,label, (50,60), cv2.FONT_HERSHEY_SCRIPT_COMPLEX,2, (120,10,200),3)    \n                                                      # putText is used to draw a detected label on image\n                                                      # (50,60)-top left coordinate   FONT_HERSHEY_SCRIPT_COMPLEX-font type\n                                                      # 2-fontscale   (120,10,200)-font colour   3-font thickness\n       \n        return img\n\nwebrtc_streamer(key=\"example\", video_transformer_factory=VideoTransformer)      \n# image captured from webcam is sent to VideoTransformer function\n#webrtc_streamer can take video_transformer_factory argument, which is a Callable that returns an instance of a class which has transform(self, frame) method.","metadata":{"_uuid":"d27c13bc-7cab-49d6-8b20-c37a7971384d","_cell_guid":"ce97880a-e59b-454b-bdcb-6a08f1c28fea","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6c9b2e56b58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maiortc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedia\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMediaPlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstreamlit_webrtc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoTransformerBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebrtc_streamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"],"ename":"ModuleNotFoundError","evalue":"No module named 'streamlit'","output_type":"error"}]}]}