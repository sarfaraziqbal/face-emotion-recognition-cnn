{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face Emotion Recognition Project","metadata":{"id":"3DR-eO17geWu"}},{"cell_type":"markdown","source":"### Importing the libraries","metadata":{"id":"EMefrVPCg-60"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nimport seaborn as sns","metadata":{"id":"sCV30xyVhFbE","trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nA good data preprocessing in machine learning is the most important factor that can make a difference between a good model and a poor machine learning model.\nThere is a concept of Garbage In Garbage Out which means that faulty & poor quality of input, even to best of computing system will produce only a bad output.\nOn the other hand if before building model, the garbage data is properly preprocessed and converted to quality, clean data even the resulting machine learning model will be of great quality.","metadata":{"id":"oxQxCBWyoGPE"}},{"cell_type":"markdown","source":"### Preprocessing the Training set","metadata":{"id":"MvE-heJNo3GG"}},{"cell_type":"code","source":"train_datagenerator = ImageDataGenerator(rescale = 1./255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\ntraining_data = train_datagenerator.flow_from_directory('../input/face-expression-recognition-dataset/images/train',\n                                                 target_size = (64, 64),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')","metadata":{"id":"0koUcJMJpEBD","trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Found 28821 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Preprocessing the Test set","metadata":{"id":"mrCMmGw9pHys"}},{"cell_type":"code","source":"validation_datagenerator = ImageDataGenerator(rescale = 1./255)\nvalidation_data = validation_datagenerator.flow_from_directory('../input/face-expression-recognition-dataset/images/validation',\n                                            target_size = (64, 64),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')","metadata":{"id":"SH4WzfOhpKc3","trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Found 7066 images belonging to 7 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Convolutional Neural Network (CNN)\n\n\nThere are three types of layers that make up the CNN which are the convolutional layers,  pooling layers, and fully-connected (FC) layers.The flatenning layer is used to flatten the input to a vector and output layer to show output. When these layers are stacked, a CNN architecture will be formed. In addition to these three layers, there are two more important parameters which are the dropout layer and the activation function","metadata":{"id":"af8O4l90gk7B"}},{"cell_type":"markdown","source":"### Initialisation","metadata":{"id":"ces1gXY2lmoX"}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()","metadata":{"id":"SAUt4UMPlhLS","trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Convolution Layer\n\n\nThis layer is the first layer that is used to extract the various features from the input images. In this layer, the mathematical operation of convolution is performed between the input image and a filter of a particular size MxM. By sliding the filter over the input image, the dot product is taken between the filter and the parts of the input image with respect to the size of the filter (MxM).T","metadata":{"id":"u5YJj_XMl5LF"}},{"cell_type":"code","source":"model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64,64,3]))","metadata":{"id":"XPzPrMckl-hV","trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Pooling Layer\n\n\nThe primary aim of this layer is to decrease the size of the convolved feature map to reduce the  computational costs. This is performed by decreasing the connections between layers and independently operates on each feature map.","metadata":{"id":"tf87FpvxmNOJ"}},{"cell_type":"code","source":"model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))","metadata":{"id":"ncpqPl69mOac","trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Activation Functions\n\n\n\nOne of the most important parameters of the CNN model is the activation function. They are used to learn and approximate any kind of continuous and complex relationship between variables of the network. In simple words, it decides which information of the model should fire in the forward direction and which ones should not at the end of the network.\n\nIt adds non-linearity to the network. There are several commonly used activation functions such as the ReLU, Softmax, tanH and the Sigmoid functions. ","metadata":{}},{"cell_type":"markdown","source":"### Adding another convolutional layer","metadata":{"id":"xaTOgD8rm4mU"}},{"cell_type":"code","source":"model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))","metadata":{"id":"i_-FZjn_m8gk","trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"###  Flattening Layer\n\n\nAfter the flattening step is that we end up with a long vector of input data that we then pass through the artificial neural network to have it processed further.What we have after we're done with each of the steps that: Input layer for the artificial neural network (flattening)","metadata":{"id":"tmiEuvTunKfk"}},{"cell_type":"code","source":"model.add(tf.keras.layers.Flatten())","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"###  Fully Connected Layer\n\n\nThe Fully Connected (FC) layer consists of the weights and biases along with the neurons and is used to connect the neurons between two different layers. These layers are usually placed before the output layer and form the last few layers of a CNN Architecture.\n\nIn this, the input image from the previous layers are flattened and fed to the FC layer. The flattened vector then undergoes few more FC layers where the mathematical functions operations usually take place. In this stage, the classification process begins to take place.","metadata":{"id":"dAoSECOm203v"}},{"cell_type":"code","source":"model.add(tf.keras.layers.Dense(units=132, activation='relu'))","metadata":{"id":"8GtmUlLd26Nq","trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"###  Output Layer\n\n\nThe Dense layers are the ones that are mostly used for the output layers. The activation used is the ‘Softmax’ which gives a probability for each class and they sum up totally to 1. The model will make it’s prediction based on the class with highest probability. ","metadata":{"id":"yTldFvbX28Na"}},{"cell_type":"code","source":"model.add(tf.keras.layers.Dense(units=7, activation='softmax'))","metadata":{"id":"1p_Zj1Mc3Ko_","trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Training the CNN","metadata":{"id":"D6XkI90snSDl"}},{"cell_type":"markdown","source":"### Compiling the CNN","metadata":{"id":"vfrFQACEnc6i"}},{"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"id":"NALksrNQpUlJ","trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Training the CNN on the Training set and evaluating it on the Validation set","metadata":{"id":"ehS-v3MIpX2h"}},{"cell_type":"code","source":"model.fit(x = training_data, validation_data = validation_data, epochs = 42)","metadata":{"id":"XUj1W4PJptta","trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/42\n901/901 [==============================] - 195s 212ms/step - loss: 1.7708 - accuracy: 0.2705 - val_loss: 1.5292 - val_accuracy: 0.4125\nEpoch 2/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.5286 - accuracy: 0.4063 - val_loss: 1.3970 - val_accuracy: 0.4609\nEpoch 3/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.4298 - accuracy: 0.4559 - val_loss: 1.3347 - val_accuracy: 0.4918\nEpoch 4/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.3613 - accuracy: 0.4824 - val_loss: 1.3066 - val_accuracy: 0.5048\nEpoch 5/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.3175 - accuracy: 0.4992 - val_loss: 1.2762 - val_accuracy: 0.5091\nEpoch 6/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.2813 - accuracy: 0.5049 - val_loss: 1.2523 - val_accuracy: 0.5275\nEpoch 7/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.2477 - accuracy: 0.5242 - val_loss: 1.2342 - val_accuracy: 0.5345\nEpoch 8/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.2094 - accuracy: 0.5380 - val_loss: 1.2291 - val_accuracy: 0.5355\nEpoch 9/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.1953 - accuracy: 0.5500 - val_loss: 1.2139 - val_accuracy: 0.5419\nEpoch 10/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.1830 - accuracy: 0.5515 - val_loss: 1.1931 - val_accuracy: 0.5442\nEpoch 11/42\n901/901 [==============================] - 67s 74ms/step - loss: 1.1655 - accuracy: 0.5585 - val_loss: 1.2220 - val_accuracy: 0.5439\nEpoch 12/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.1530 - accuracy: 0.5617 - val_loss: 1.1904 - val_accuracy: 0.5545\nEpoch 13/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.1314 - accuracy: 0.5692 - val_loss: 1.1974 - val_accuracy: 0.5600\nEpoch 14/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.1280 - accuracy: 0.5707 - val_loss: 1.1896 - val_accuracy: 0.5559\nEpoch 15/42\n901/901 [==============================] - 60s 66ms/step - loss: 1.1225 - accuracy: 0.5756 - val_loss: 1.1671 - val_accuracy: 0.5618\nEpoch 16/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.1036 - accuracy: 0.5847 - val_loss: 1.1787 - val_accuracy: 0.5536\nEpoch 17/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.0949 - accuracy: 0.5848 - val_loss: 1.1664 - val_accuracy: 0.5604\nEpoch 18/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.0808 - accuracy: 0.5920 - val_loss: 1.1724 - val_accuracy: 0.5692\nEpoch 19/42\n901/901 [==============================] - 58s 64ms/step - loss: 1.0742 - accuracy: 0.5926 - val_loss: 1.1640 - val_accuracy: 0.5652\nEpoch 20/42\n901/901 [==============================] - 57s 63ms/step - loss: 1.0723 - accuracy: 0.5958 - val_loss: 1.1485 - val_accuracy: 0.5785\nEpoch 21/42\n901/901 [==============================] - 57s 63ms/step - loss: 1.0647 - accuracy: 0.5938 - val_loss: 1.1642 - val_accuracy: 0.5672\nEpoch 22/42\n901/901 [==============================] - 57s 63ms/step - loss: 1.0535 - accuracy: 0.6032 - val_loss: 1.1731 - val_accuracy: 0.5599\nEpoch 23/42\n901/901 [==============================] - 57s 64ms/step - loss: 1.0397 - accuracy: 0.6063 - val_loss: 1.1513 - val_accuracy: 0.5706\nEpoch 24/42\n901/901 [==============================] - 57s 64ms/step - loss: 1.0436 - accuracy: 0.6082 - val_loss: 1.1628 - val_accuracy: 0.5685\nEpoch 25/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.0406 - accuracy: 0.6086 - val_loss: 1.1751 - val_accuracy: 0.5729\nEpoch 26/42\n901/901 [==============================] - 58s 65ms/step - loss: 1.0260 - accuracy: 0.6104 - val_loss: 1.1478 - val_accuracy: 0.5718\nEpoch 27/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.0146 - accuracy: 0.6195 - val_loss: 1.1986 - val_accuracy: 0.5613\nEpoch 28/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.0294 - accuracy: 0.6099 - val_loss: 1.1690 - val_accuracy: 0.5720\nEpoch 29/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.0102 - accuracy: 0.6193 - val_loss: 1.1605 - val_accuracy: 0.5732\nEpoch 30/42\n901/901 [==============================] - 59s 65ms/step - loss: 0.9977 - accuracy: 0.6242 - val_loss: 1.1823 - val_accuracy: 0.5637\nEpoch 31/42\n901/901 [==============================] - 58s 65ms/step - loss: 0.9918 - accuracy: 0.6252 - val_loss: 1.1614 - val_accuracy: 0.5742\nEpoch 32/42\n901/901 [==============================] - 59s 65ms/step - loss: 1.0008 - accuracy: 0.6174 - val_loss: 1.1774 - val_accuracy: 0.5659\nEpoch 33/42\n901/901 [==============================] - 58s 65ms/step - loss: 0.9892 - accuracy: 0.6289 - val_loss: 1.1878 - val_accuracy: 0.5730\nEpoch 34/42\n901/901 [==============================] - 58s 64ms/step - loss: 0.9833 - accuracy: 0.6332 - val_loss: 1.1798 - val_accuracy: 0.5805\nEpoch 35/42\n901/901 [==============================] - 57s 64ms/step - loss: 0.9735 - accuracy: 0.6356 - val_loss: 1.1773 - val_accuracy: 0.5709\nEpoch 36/42\n901/901 [==============================] - 58s 65ms/step - loss: 0.9665 - accuracy: 0.6368 - val_loss: 1.1733 - val_accuracy: 0.5708\nEpoch 37/42\n901/901 [==============================] - 58s 65ms/step - loss: 0.9804 - accuracy: 0.6334 - val_loss: 1.1824 - val_accuracy: 0.5784\nEpoch 38/42\n901/901 [==============================] - 60s 67ms/step - loss: 0.9645 - accuracy: 0.6362 - val_loss: 1.2029 - val_accuracy: 0.5735\nEpoch 39/42\n901/901 [==============================] - 58s 64ms/step - loss: 0.9573 - accuracy: 0.6372 - val_loss: 1.1750 - val_accuracy: 0.5791\nEpoch 40/42\n901/901 [==============================] - 58s 64ms/step - loss: 0.9581 - accuracy: 0.6444 - val_loss: 1.1839 - val_accuracy: 0.5812\nEpoch 41/42\n901/901 [==============================] - 58s 64ms/step - loss: 0.9375 - accuracy: 0.6468 - val_loss: 1.2072 - val_accuracy: 0.5743\nEpoch 42/42\n901/901 [==============================] - 58s 65ms/step - loss: 0.9405 - accuracy: 0.6455 - val_loss: 1.1797 - val_accuracy: 0.5778\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fc948896950>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saving model","metadata":{}},{"cell_type":"code","source":"model.save('model1.h5')","metadata":{"id":"ED9KB3I54c1i","trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Single Value prediction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.preprocessing import image\ntest_image = image.load_img('../input/happyman/82.jpg', target_size = (64, 64))\ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis = 0)\nresult = model.predict(test_image)\ntraining_data.class_indices\nif result[0][0] == 1:\n  prediction = '11'\nelif result[0][1] == 2:\n  prediction = '22'\nelif result[0][2] == 3:\n  prediction = '33'\nelif result[0][3] == 4:\n  prediction = '44'\nelif result[0][4] == 5:\n  prediction = '55'\nelif result[0][5] == 6:\n  prediction = '66'\nelse:\n  prediction = '77'","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(result)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[[0. 0. 0. 1. 0. 0. 0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"max_value = max(result)","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"max_index_col = np.argmax(result, axis=1)","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"max_index_col","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"array([3])"},"metadata":{}}]}]}